{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from absl import app\n",
    "from dotenv import load_dotenv\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import neptune\n",
    "import numpy as np\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, NamedTuple, Tuple, Callable\n",
    "from learntrix.training.losses import cross_entropy_loss\n",
    "\n",
    "from learntrix.dataloaders.computer_vision.mnist import load_mnist_dataset\n",
    "\n",
    "from learntrix.types import Batch, TrainingState, Metrics\n",
    "\n",
    "# Set default device to CPU for JAX\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get devices if any\n",
    "devices = jax.devices(\"cpu\")\n",
    "num_devices = len(devices)\n",
    "print(f\"Detected the following devices: {tuple(devices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_variable(path, name):\n",
    "    load_dotenv(path)\n",
    "    variable = os.getenv(name)\n",
    "    return variable\n",
    "\n",
    "def run_neptune(path, project):\n",
    "    \"\"\"\n",
    "    path: path of env file with Neptune token\n",
    "    neptune_project: name of the neptune project\n",
    "    \"\"\"\n",
    "    api_token = load_env_variable(path=path, name='NEPTUNE_API_TOKEN')\n",
    "\n",
    "    run = neptune.init_run(\n",
    "        project=project,\n",
    "        api_token=api_token,\n",
    "    )\n",
    "\n",
    "    return run\n",
    "\n",
    "run = run_neptune(path='./.env', project=\"yanisadel/learn-jax\")\n",
    "\n",
    "params = {\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
    "run[\"parameters\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_mnist_dataset(\n",
    "    \"train\",\n",
    "    shuffle=True, \n",
    "    batch_size=64\n",
    "    )\n",
    "data_test = load_mnist_dataset(\n",
    "    \"test\",\n",
    "    shuffle=False, \n",
    "    batch_size=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(x: jax.Array) -> jax.Array:\n",
    "    x = x.astype(jnp.float32) / 255.\n",
    "    mlp = hk.Sequential([\n",
    "        hk.Flatten(),\n",
    "        hk.Linear(300), jax.nn.relu,\n",
    "        hk.Linear(100), jax.nn.relu,\n",
    "        hk.Linear(10),\n",
    "    ])\n",
    "    return mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, forward_fn, loss_fn, optimizer, num_classes):\n",
    "        self._forward_fn = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "        self._loss_fn = partial(loss_fn,\n",
    "                                forward_fn=self._forward_fn.apply,\n",
    "                                num_classes=num_classes)\n",
    "        self._optimizer = optimizer\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "    \n",
    "    def init(self, random_key, x):\n",
    "        params = self._forward_fn.init(random_key, x)\n",
    "        opt_state = self._optimizer.init(params)\n",
    "\n",
    "        return TrainingState(params, opt_state)\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def update(self, state: TrainingState, batch: Batch) -> Tuple[TrainingState, Metrics]:\n",
    "        (_, metrics), grads = jax.value_and_grad(self._loss_fn, has_aux=True)(state.params, batch)\n",
    "        grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "        updates, opt_state = self._optimizer.update(grads, state.opt_state)\n",
    "        params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "        return TrainingState(params, opt_state), metrics\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def evaluate(self, state: TrainingState, batch: Batch):\n",
    "        _, metrics = self._loss_fn(state.params, batch)\n",
    "    \n",
    "        return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    forward_fn=forward_fn,\n",
    "    loss_fn=cross_entropy_loss,\n",
    "    optimizer=optax.adam(learning_rate=1e-3),\n",
    "    num_classes=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state = trainer.init(\n",
    "    jax.random.PRNGKey(0), \n",
    "    x=jnp.ones(shape=(32, 28, 28, 1))\n",
    "    )\n",
    "training_state = jax.device_put_replicated(training_state, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(update_fn, evaluate_fn, state, devices, dataset_train, dataset_test, num_steps, validation_step, run_neptune):\n",
    "    all_metrics: Metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_step\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_step\": [],\n",
    "    }\n",
    "\n",
    "    batch_test = next(dataset_test)\n",
    "    batch_test = jax.device_put_replicated(batch_test, devices)\n",
    "\n",
    "    for step in tqdm(range(1, num_steps+1)):\n",
    "        batch = next(dataset_train)\n",
    "        batch = jax.device_put_replicated(batch, devices)\n",
    "        # batch = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch)\n",
    "\n",
    "        state, metrics = jax.pmap(update_fn, devices=devices, axis_name='batch')(state, batch)\n",
    "        \n",
    "        train_loss = jax.device_get(metrics[\"loss\"]).mean()\n",
    "        train_accuracy = jax.device_get(metrics[\"accuracy\"]).mean()\n",
    "\n",
    "        run_neptune[\"train/loss\"].log(train_loss, step=step)\n",
    "        run_neptune[\"train/accuracy\"].log(train_accuracy, step=step)\n",
    "\n",
    "        all_metrics[\"train_loss\"].append(train_loss)\n",
    "        all_metrics[\"train_acc\"].append(train_accuracy)\n",
    "        all_metrics[\"train_step\"].append(step)\n",
    "\n",
    "        if step % validation_step == 0:\n",
    "            # batch_test = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch_test)\n",
    "            metrics = jax.pmap(trainer.evaluate, devices=devices, axis_name='batch')(state, batch_test)\n",
    "            test_loss = jax.device_get(metrics[\"loss\"]).mean()\n",
    "            test_accuracy = jax.device_get(metrics[\"accuracy\"]).mean()\n",
    "\n",
    "            run_neptune[\"test/loss\"].log(test_loss, step=step)\n",
    "            run_neptune[\"test/accuracy\"].log(test_accuracy, step=step)\n",
    "\n",
    "            all_metrics[\"val_loss\"].append(test_loss)\n",
    "            all_metrics[\"val_acc\"].append(test_accuracy)\n",
    "            all_metrics[\"val_step\"].append(step)\n",
    "    \n",
    "    return state, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, metrics = run_train(trainer.update, trainer.evaluate, state=training_state, devices=devices, dataset_train=data_train, dataset_test=data_test, num_steps=100, validation_step=10, run_neptune=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

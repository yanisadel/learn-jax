{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from collections.abc import Iterator\n",
    "from typing import NamedTuple\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Set default device to CPU for JAX\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get devices if any\n",
    "devices = jax.devices(\"cpu\")\n",
    "num_devices = len(devices)\n",
    "print(f\"Detected the following devices: {tuple(devices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "  image: np.ndarray  # [B, H, W, 1]\n",
    "  label: np.ndarray  # [B]\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "  params: hk.Params\n",
    "  opt_state: optax.OptState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    split: str,\n",
    "    *,\n",
    "    shuffle: bool,\n",
    "    batch_size: int,\n",
    ") -> Iterator[Batch]:\n",
    "  \"\"\"Loads the MNIST dataset.\"\"\"\n",
    "  ds, ds_info = tfds.load(\"mnist:3.*.*\", split=split, with_info=True)\n",
    "  ds.cache()\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(ds_info.splits[split].num_examples, seed=0)\n",
    "  ds = ds.repeat()\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.map(lambda x: Batch(**x))\n",
    "  return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    def forward_fn(x: jax.Array) -> jax.Array:\n",
    "        x = x.astype(jnp.float32) / 255.\n",
    "        mlp = hk.Sequential([\n",
    "            hk.Flatten(),\n",
    "            hk.Linear(300), jax.nn.relu,\n",
    "            hk.Linear(100), jax.nn.relu,\n",
    "            hk.Linear(10),\n",
    "        ])\n",
    "        return mlp(x)\n",
    "    \n",
    "    def loss_fn(params, batch):\n",
    "        logits = network.apply(params, batch.image)\n",
    "        labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
    "        loss_value = -jnp.sum(labels * jax.nn.log_softmax(logits)) / batch.image.shape[0]\n",
    "        return loss_value\n",
    "\n",
    "    @partial(jax.pmap, devices=devices, axis_name='batch')\n",
    "    def update(state, batch):\n",
    "        grads = jax.grad(loss_fn)(state.params, batch)\n",
    "        grads = jax.lax.pmean(grads, axis_name='batch') # pmap\n",
    "        updates, opt_state = optimizer.update(grads, state.opt_state)\n",
    "        params = optax.apply_updates(state.params, updates)\n",
    "        return TrainingState(params, opt_state)\n",
    "    \n",
    "    @partial(jax.pmap, devices=devices, axis_name='batch')\n",
    "    def evaluate(state, batch):\n",
    "        logits = network.apply(state.params, batch.image)\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        accuracy = jnp.mean(predictions == batch.label)\n",
    "        return jax.lax.pmean(accuracy, axis_name='batch') # pmap\n",
    "\n",
    "    NUM_CLASSES = 10\n",
    "    BATCH_SIZE_TRAIN = 64\n",
    "    BATCH_SIZE_TEST = 10000\n",
    "\n",
    "    print(\"Initializing network..\")\n",
    "    network = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "    print(\"Initializing data..\")\n",
    "    data_train = load_dataset(\"train\", shuffle=True, batch_size=BATCH_SIZE_TRAIN)\n",
    "    data_test = load_dataset(\"test\", shuffle=False, batch_size=BATCH_SIZE_TEST)\n",
    "\n",
    "    print(\"Initializing parameters..\")\n",
    "    params = network.init(jax.random.PRNGKey(seed=0), next(data_train).image)\n",
    "    optimizer = optax.adam(learning_rate=1e-4)\n",
    "    opt_state = optimizer.init(params)\n",
    "    state = TrainingState(params, opt_state)\n",
    "\n",
    "    # Replicate parameters and optimizer state across devices\n",
    "    state = jax.device_put_replicated(state, devices) # pmap\n",
    "\n",
    "    print(\"Training..\")\n",
    "    NUM_EPOCHS = 5\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        for i in range(100):\n",
    "            batch = next(data_train)\n",
    "            # Split batch for pmap\n",
    "            # batch = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch)\n",
    "            batch = jax.device_put_replicated(batch, devices)\n",
    "            state = update(state, batch)\n",
    "        \n",
    "        # Evaluate on test batch\n",
    "        batch_test = next(data_test)\n",
    "        batch_test = jax.device_put_replicated(batch_test, devices) # I don't know\n",
    "        # batch_test = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch_test)\n",
    "        accuracy = evaluate(state, batch_test)\n",
    "        accuracy = jax.device_get(accuracy)\n",
    "        print(\"Accuracy : \", accuracy.mean())\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

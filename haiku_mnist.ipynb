{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from collections.abc import Iterator\n",
    "from typing import NamedTuple\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from typing import Dict\n",
    "from typing_extensions import TypeAlias\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import neptune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "# Set default device to CPU for JAX\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get devices if any\n",
    "devices = jax.devices(\"cpu\")\n",
    "num_devices = len(devices)\n",
    "print(f\"Detected the following devices: {tuple(devices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "api_token = os.getenv(\"NEPTUNE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"yanisadel/learn-jax\",\n",
    "    api_token=api_token,\n",
    ")\n",
    "\n",
    "params = {\"learning_rate\": 0.001, \"optimizer\": \"Adam\"}\n",
    "run[\"parameters\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(NamedTuple):\n",
    "  image: np.ndarray  # [B, H, W, 1]\n",
    "  label: np.ndarray  # [B]\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "  params: hk.Params\n",
    "  opt_state: optax.OptState\n",
    "\n",
    "Metrics: TypeAlias = Dict[str, jnp.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    split: str,\n",
    "    *,\n",
    "    shuffle: bool,\n",
    "    batch_size: int,\n",
    ") -> Iterator[Batch]:\n",
    "  \"\"\"Loads the MNIST dataset.\"\"\"\n",
    "  ds, ds_info = tfds.load(\"mnist:3.*.*\", split=split, with_info=True)\n",
    "  ds.cache()\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(ds_info.splits[split].num_examples, seed=0)\n",
    "  ds = ds.repeat()\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.map(lambda x: Batch(**x))\n",
    "  return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    def forward_fn(x: jax.Array) -> jax.Array:\n",
    "        x = x.astype(jnp.float32) / 255.\n",
    "        mlp = hk.Sequential([\n",
    "            hk.Flatten(),\n",
    "            hk.Linear(300), jax.nn.relu,\n",
    "            hk.Linear(100), jax.nn.relu,\n",
    "            hk.Linear(10),\n",
    "        ])\n",
    "        return mlp(x)\n",
    "    \n",
    "    def loss_fn(params: hk.Params, batch: NamedTuple):\n",
    "        logits = network.apply(params, batch.image)\n",
    "        labels = jax.nn.one_hot(batch.label, NUM_CLASSES)\n",
    "        loss_value = -jnp.sum(labels * jax.nn.log_softmax(logits)) / batch.image.shape[0]\n",
    "\n",
    "        predictions_proba = jax.nn.softmax(logits, axis=-1)\n",
    "        predictions = jnp.argmax(predictions_proba, axis=-1)\n",
    "        accuracy = jnp.mean(predictions == batch.label)\n",
    "        pmean_accuracy = jax.lax.pmean(accuracy, axis_name='batch') # Faudra que je vÃ©rifie quand num_devices > 1\n",
    "\n",
    "        metrics: Metrics = {\n",
    "            \"predictions\": predictions_proba,\n",
    "            \"loss\": loss_value,\n",
    "            \"accuracy\": pmean_accuracy\n",
    "        }\n",
    "\n",
    "        return loss_value, metrics\n",
    "\n",
    "    @partial(jax.pmap, devices=devices, axis_name='batch')\n",
    "    def update(state: NamedTuple, batch: NamedTuple) -> Tuple[NamedTuple, Metrics]:\n",
    "        (_, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params, batch)\n",
    "        grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "        updates, opt_state = optimizer.update(grads, state.opt_state)\n",
    "        params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "        return TrainingState(params, opt_state), metrics\n",
    "    \n",
    "    @partial(jax.pmap, devices=devices, axis_name='batch')\n",
    "    def evaluate(state: NamedTuple, batch: NamedTuple):\n",
    "        _, metrics = loss_fn(state.params, batch)\n",
    "        # predictions_proba = metrics[\"predictions\"]\n",
    "        # predictions = jnp.argmax(predictions_proba, axis=-1)\n",
    "        # accuracy = jnp.mean(predictions == batch.label)\n",
    "        # pmean_accuracy = jax.lax.pmean(accuracy, axis_name='batch') # log\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    NUM_CLASSES = 10\n",
    "    BATCH_SIZE_TRAIN = 64\n",
    "    BATCH_SIZE_TEST = 10000\n",
    "\n",
    "    print(\"Initializing network..\")\n",
    "    network = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "\n",
    "    print(\"Initializing data..\")\n",
    "    data_train = load_dataset(\"train\", shuffle=True, batch_size=BATCH_SIZE_TRAIN)\n",
    "    data_test = load_dataset(\"test\", shuffle=False, batch_size=BATCH_SIZE_TEST)\n",
    "\n",
    "    print(\"Initializing parameters..\")\n",
    "    params = network.init(jax.random.PRNGKey(seed=0), next(data_train).image)\n",
    "    optimizer = optax.adam(learning_rate=1e-4)\n",
    "    opt_state = optimizer.init(params)\n",
    "    state = TrainingState(params, opt_state)\n",
    "\n",
    "    # Replicate parameters and optimizer state across devices\n",
    "    state = jax.device_put_replicated(state, devices)\n",
    "\n",
    "    batch_test = next(data_test)\n",
    "    batch_test = jax.device_put_replicated(batch_test, devices)\n",
    "\n",
    "    all_metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_step\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_step\": [],\n",
    "    }\n",
    "    \n",
    "    print(\"Training..\")\n",
    "    NUM_STEPS = 100\n",
    "    validation_step = 10\n",
    "\n",
    "    for step in tqdm(range(1, NUM_STEPS+1)):\n",
    "        batch = next(data_train)\n",
    "        # batch = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch)\n",
    "        batch = jax.device_put_replicated(batch, devices)\n",
    "        state, metrics = update(state, batch)\n",
    "        \n",
    "        train_loss = jax.device_get(metrics[\"loss\"]).mean()\n",
    "        train_accuracy = jax.device_get(metrics[\"accuracy\"]).mean()\n",
    "\n",
    "        run[\"train/loss\"].log(train_loss, step=step)\n",
    "        run[\"train/accuracy\"].log(train_accuracy, step=step)\n",
    "\n",
    "        all_metrics[\"train_loss\"].append(train_loss)\n",
    "        all_metrics[\"train_acc\"].append(train_accuracy)\n",
    "        all_metrics[\"train_step\"].append(step)\n",
    "\n",
    "        if step % validation_step == 0:\n",
    "            # Evaluate on test batch\n",
    "            \n",
    "            # batch_test = jax.tree.map(lambda x: x.reshape((num_devices, -1) + x.shape[1:]), batch_test)\n",
    "            metrics = evaluate(state, batch_test)\n",
    "            test_loss = jax.device_get(metrics[\"loss\"]).mean()\n",
    "            test_accuracy = jax.device_get(metrics[\"accuracy\"]).mean()\n",
    "\n",
    "            run[\"test/loss\"].log(test_loss, step=step)\n",
    "            run[\"test/accuracy\"].log(test_accuracy, step=step)\n",
    "\n",
    "            all_metrics[\"val_loss\"].append(test_loss)\n",
    "            all_metrics[\"val_acc\"].append(test_accuracy)\n",
    "            all_metrics[\"val_step\"].append(step)\n",
    "\n",
    "    run.stop()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
